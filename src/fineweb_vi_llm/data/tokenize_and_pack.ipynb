{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:23.673995Z",
     "iopub.status.busy": "2025-08-31T14:11:23.673581Z",
     "iopub.status.idle": "2025-08-31T14:11:23.680481Z",
     "shell.execute_reply": "2025-08-31T14:11:23.679054Z",
     "shell.execute_reply.started": "2025-08-31T14:11:23.673969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:23.682654Z",
     "iopub.status.busy": "2025-08-31T14:11:23.682208Z",
     "iopub.status.idle": "2025-08-31T14:11:24.249442Z",
     "shell.execute_reply": "2025-08-31T14:11:24.248213Z",
     "shell.execute_reply.started": "2025-08-31T14:11:23.682624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fw_tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained('thng292/fineweb-vi-en-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:24.255919Z",
     "iopub.status.busy": "2025-08-31T14:11:24.255552Z",
     "iopub.status.idle": "2025-08-31T14:11:24.263728Z",
     "shell.execute_reply": "2025-08-31T14:11:24.262435Z",
     "shell.execute_reply.started": "2025-08-31T14:11:24.255879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fw_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:24.265398Z",
     "iopub.status.busy": "2025-08-31T14:11:24.264913Z",
     "iopub.status.idle": "2025-08-31T14:11:27.790550Z",
     "shell.execute_reply": "2025-08-31T14:11:27.789301Z",
     "shell.execute_reply.started": "2025-08-31T14:11:24.265370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_en: datasets.DatasetDict = datasets.load_dataset(\"thng292/fineweb-subset-1M\", \"subset-en\")\n",
    "data_vi: datasets.DatasetDict = datasets.load_dataset(\"thng292/fineweb-subset-1M\", \"subset-vi\")\n",
    "\n",
    "train = datasets.interleave_datasets([data_en[\"train\"], data_vi[\"train\"]])\n",
    "test = datasets.interleave_datasets([data_en[\"test\"], data_vi[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:27.793366Z",
     "iopub.status.busy": "2025-08-31T14:11:27.793039Z",
     "iopub.status.idle": "2025-08-31T14:11:27.798429Z",
     "shell.execute_reply": "2025-08-31T14:11:27.797286Z",
     "shell.execute_reply.started": "2025-08-31T14:11:27.793344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_len = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:46:23.434969Z",
     "iopub.status.busy": "2025-08-31T14:46:23.434562Z",
     "iopub.status.idle": "2025-08-31T14:46:23.447441Z",
     "shell.execute_reply": "2025-08-31T14:46:23.446317Z",
     "shell.execute_reply.started": "2025-08-31T14:46:23.434944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "import gc\n",
    "\n",
    "\n",
    "def search_for_fit(numbers: list[tuple[int, int]], capacity: int) -> int:\n",
    "    r\"\"\"Find the index of largest number that fits into the knapsack with the given capacity.\"\"\"\n",
    "    index = bisect.bisect(numbers, capacity, key=lambda a: a[1])\n",
    "    return index - 1\n",
    "\n",
    "\n",
    "def greedy_knapsack(\n",
    "    numbers: list[tuple[int, int]],  # should be `list(enumerate(lengths + 1))`\n",
    "    capacity: int,\n",
    ") -> list[list[tuple[int, int]]]:\n",
    "    r\"\"\"Implement efficient greedy algorithm with binary search for the knapsack problem.\"\"\"\n",
    "    numbers.sort(\n",
    "        key=lambda a: a[1]\n",
    "    )  # sort numbers in ascending order for binary search\n",
    "    knapsacks = []\n",
    "\n",
    "    while numbers:\n",
    "        current_knapsack = []\n",
    "        remaining_capacity = capacity\n",
    "\n",
    "        while True:\n",
    "            index = search_for_fit(numbers, remaining_capacity)\n",
    "            if index == -1:\n",
    "                break  # no more numbers fit in this knapsack\n",
    "\n",
    "            remaining_capacity -= numbers[index][1]  # update the remaining capacity\n",
    "            current_knapsack.append(numbers.pop(index))  # add the number to knapsack\n",
    "\n",
    "        knapsacks.append(current_knapsack)\n",
    "\n",
    "    return knapsacks\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizeAndPack:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    max_length: int\n",
    "\n",
    "    def __call__(self, row: dict[str, list]):\n",
    "        texts: list[str] = row[\"text\"]\n",
    "        tokens = self.tokenizer(texts)\n",
    "\n",
    "        tokenss = [\n",
    "            input_ids\n",
    "            for input_ids in tokens.data[\"input_ids\"]\n",
    "            if len(input_ids) < self.max_length\n",
    "        ]\n",
    "        lengths = [len(token) for token in tokenss]\n",
    "\n",
    "        input_idss: list[list[int]] = []\n",
    "        attention_masks: list[list[int]] = []\n",
    "        # pad_len = max(lengths) + 1\n",
    "\n",
    "        to_be_packed = greedy_knapsack(\n",
    "            list(enumerate(length + 1 for length in lengths)), capacity=self.max_length\n",
    "        )\n",
    "        for packed_sample_ids in to_be_packed:\n",
    "            input_ids = []\n",
    "            attention_mask = []\n",
    "            seq_count = 0\n",
    "            for index, length in packed_sample_ids:\n",
    "                input_ids.extend(tokenss[index] + [self.tokenizer.eos_token_id])\n",
    "                attention_mask.extend([seq_count] * length)\n",
    "                assert len(input_ids) == len(attention_mask)\n",
    "                seq_count += 1\n",
    "            # to_pad = pad_len - len(input_ids)\n",
    "            input_idss.append(input_ids)  # + [tokenizer.pad_token_id] * to_pad\n",
    "            attention_masks.append(attention_mask)  # + [seq_count] * to_pad\n",
    "        gc.collect()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_idss,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"length\": [len(input_ids) for input_ids in input_idss],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:11:43.872409Z",
     "iopub.status.busy": "2025-08-31T14:11:43.872071Z",
     "iopub.status.idle": "2025-08-31T14:24:07.133896Z",
     "shell.execute_reply": "2025-08-31T14:24:07.132488Z",
     "shell.execute_reply.started": "2025-08-31T14:11:43.872389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = train.map(\n",
    "    TokenizeAndPack(fw_tokenizer, max_length=max_len),\n",
    "    fn_kwargs={\"max_length\": max_len},\n",
    "    num_proc=os.cpu_count() or 1,\n",
    "    batched=True,\n",
    "    remove_columns=train.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T14:11:27.854777Z",
     "iopub.status.idle": "2025-08-31T14:11:27.855104Z",
     "shell.execute_reply": "2025-08-31T14:11:27.854979Z",
     "shell.execute_reply.started": "2025-08-31T14:11:27.854967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test = test.map(\n",
    "    TokenizeAndPack(fw_tokenizer, max_length=max_len), \n",
    "    fn_kwargs={\"max_length\": max_len},\n",
    "    num_proc=os.cpu_count() or 1, \n",
    "    batched=True,\n",
    "    remove_columns=test.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T14:11:27.856821Z",
     "iopub.status.idle": "2025-08-31T14:11:27.857155Z",
     "shell.execute_reply": "2025-08-31T14:11:27.857037Z",
     "shell.execute_reply.started": "2025-08-31T14:11:27.857024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T14:11:27.859531Z",
     "iopub.status.idle": "2025-08-31T14:11:27.859960Z",
     "shell.execute_reply": "2025-08-31T14:11:27.859773Z",
     "shell.execute_reply.started": "2025-08-31T14:11:27.859755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "experiment_1 = datasets.DatasetDict()\n",
    "experiment_1[\"train\"] = tokenized_train \n",
    "experiment_1[\"test\"] = tokenized_test\n",
    "experiment_1.push_to_hub(\"fw-experiment-1-tokenized-packed\", token=secret_value_0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "fineweb-vi-llm (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
